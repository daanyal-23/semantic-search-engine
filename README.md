# ğŸ” Semantic Search Engine
## AI Engineer Internship Assignment â€” CodeAtRandom

This project implements a complete semantic document retrieval system using:

-MiniLM SentenceTransformer embeddings (384-dim)

-FAISS vector search (Inner Product index)

-Caching system to avoid redundant embeddings

-FastAPI backend with /search endpoint

-Ranking explanation (keyword overlap, score, normalization)

-Streamlit UI (Bonus)

This repository includes a full working pipeline:
preprocessing â†’ embeddings + caching â†’ FAISS index â†’ search â†’ ranking â†’ API â†’ UI.

# ğŸš€ Features Overview
## âœ” Task 1: Preprocessing

-Download 20 Newsgroups dataset

-Clean + normalize text

-Save first 200 documents

## âœ” Task 2A: Embedding Generator

-MiniLM-L6-v2 embeddings

-Normalized vectors for cosine similarity

-Batch encoding

## âœ” Task 2B: Cache Manager

-JSON-based cache (doc_id, embedding, hash, timestamp)

-Only recompute embeddings if file changes

## âœ” Task 3: Vector Database (FAISS)

-Build + persist FAISS index (vector_index.faiss)

-Maintain ID-to-doc mapping (id_map.json)

-Load index instantly for searching

## âœ” Task 4: Retrieval API

-Built with FastAPI

-/search endpoint

-Input: {query, top_k}

-Output: Top-k ranked results with explanations

## âœ” Task 5: Ranking Explanation

Each result includes:

-Why it matched

-Keyword overlap

-Overlap ratio

-Document length normalization score

# â­ Bonus Features (Implemented)

-Persistent FAISS index

-Streamlit UI interface (streamlit_app.py)

# ğŸ“ Folder Structure
```bash
semantic-search-engine/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py                 # FastAPI app
â”‚   â”œâ”€â”€ preprocess.py          # Download + clean dataset
â”‚   â”œâ”€â”€ create_metadata.py     # Build metadata.json
â”‚   â”œâ”€â”€ embedder.py            # Embedding utilities
â”‚   â”œâ”€â”€ cache_manager.py       # JSON embedding cache
â”‚   â”œâ”€â”€ search_engine.py       # FAISS index builder + loader
â”‚   â”œâ”€â”€ ranker.py              # Ranking + scoring logic
â”‚   â”œâ”€â”€ explainer.py           # Match explanation generator
â”‚   â”œâ”€â”€ query_pipeline.py      # Full query â†’ results flow
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ streamlit_app.py           # Bonus UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
## ğŸ“Œ Ignored (per assignment)

-data/

-cache/

-vector_store/

-models/

## virtual environments
```bash
â”œâ”€â”€ streamlit_app.py           # Bonus UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
## ğŸ“Œ Ignored (per assignment)
-data/

-cache/

-vector_store/

-models/

-virtual environments

# ğŸ§  How Caching Works
Caching is handled in src/cache_manager.py.

For each document:

-Field	Purpose
-doc_id	Unique document ID
-embedding	384-dim MiniLM vector
-hash	SHA-256 of document text
-updated_at	Timestamp

### How the system uses the cache

-Compute SHA-256 of doc text

-If doc exists in cache and hash matches â†’ reuse embedding

-If hash changed or missing â†’ compute new embedding

-Save to cache/embeddings.json

âœ” Saves massive processing time
âœ” Only re-embeds changed files
âœ” Exactly matches assignment requirements

Field	Purpose
doc_id	Unique document ID
embedding	384-dim MiniLM vector
hash	SHA-256 of document text
updated_at	Timestamp

### How the system uses the cache

-Compute SHA-256 of doc text

-If doc exists in cache and hash matches â†’ reuse embedding

-If hash changed or missing â†’ compute new embedding

-Save to cache/embeddings.json

âœ” Saves massive processing time
âœ” Only re-embeds changed files
âœ” Exactly matches assignment requirements

# âš™ï¸ How to Generate Embeddings & Build FAISS Index
Step 1 â†’ Preprocess documents
```bash
python -m src.preprocess
```
Creates:
```bash
data/docs/doc_001.txt ...
data/metadata.json
```
Step 2 â†’ Build FAISS index

Open Python:
```bash
from src.search_engine import SearchEngine
se = SearchEngine()
se.build_index()
```

Produces:
```bash
vector_store/vector_index.faiss
vector_store/id_map.json
```

Step 3 â†’ Test search engine
```bash
se.load_index()
results = se.search("machine learning", top_k=5)
print(results)
```

# ğŸŒ Starting the FastAPI Server

Run:

uvicorn src.api:app --reload --host 0.0.0.0 --port 8000


Open:

## ğŸ‘‰ http://127.0.0.1:8000/docs

Test:

{
  "query": "quantum physics basics",
  "top_k": 5
}


## ğŸ” Sample Search Response
{
  "doc_id": "doc_083",
  "score": 0.2705,
  "preview": "australian pattern recognition...",
  "explanation": {
    "why_matched": "Matched because query keywords machine appear in the document.",
    "overlap_keywords": ["machine"],
    "overlap_ratio": 0.3333,
    "doc_length_norm": 0.1376
  }
}


# ğŸ–¥ Streamlit UI (Bonus)

Run:
```bash
streamlit run streamlit_app.py
```

Opens at:

## ğŸ‘‰ http://localhost:8501

Features:

-Search bar

-Top-K slider

-Document results

-Explanation expandable panel

-Clean and simple UI


# ğŸ§ª Design Choices (Why This Architecture?)
âœ” MiniLM-L6-v2

Fast, lightweight, high-quality sentence embeddings.

âœ” FAISS Inner Product Index

Efficient cosine-similarity search for up to millions of vectors.

âœ” JSON Cache

Readable, portable, simple to debug.

âœ” FastAPI

Modern, async-first, built-in docs.

âœ” Streamlit

Zero-friction UI for demonstrations.

âœ” Modular Architecture

Each module handles one responsibility, making the system clean and extensible.


# ğŸ“¦ Installation
pip install -r requirements.txt

# ğŸ§ª Future Improvements

ğŸ”§ Add multiprocessing for batch embedding
ğŸ”§ Use ONNX runtime for faster embeddings
ğŸ”§ Replace FAISS with Weaviate / Milvus
ğŸ”§ Add hybrid retrieval (BM25 + vectors)
ğŸ”§ Add evaluation metrics (nDCG, recall@k)
ğŸ”§ Dockerize for deployment

# ğŸ¤ Contributing

Pull requests are welcome!
For major changes, open an issue first to discuss improvements.

â¤ï¸ Made with Love â€” and FAISS, Transformers, FastAPI, and Streamlit
